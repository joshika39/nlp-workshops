{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a sentiment analysis pipeline with HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.34.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#for colab\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "classifier = pipeline(\"text-classification\", model=model_name, top_k=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a \"Sentiment Analysis\" **classifier** using the pipeline function provided by the Hugging Face Transformers library. This function allows us to easily use pre-trained models for various natural language processing (NLP) tasks, like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'neutral', 'score': 0.7125529646873474},\n",
       "  {'label': 'approval', 'score': 0.13431048393249512},\n",
       "  {'label': 'realization', 'score': 0.07332759350538254},\n",
       "  {'label': 'joy', 'score': 0.05254011228680611},\n",
       "  {'label': 'relief', 'score': 0.026701610535383224},\n",
       "  {'label': 'optimism', 'score': 0.012795311398804188},\n",
       "  {'label': 'excitement', 'score': 0.011233342811465263},\n",
       "  {'label': 'pride', 'score': 0.009617138653993607},\n",
       "  {'label': 'caring', 'score': 0.009234932251274586},\n",
       "  {'label': 'sadness', 'score': 0.008024858310818672},\n",
       "  {'label': 'annoyance', 'score': 0.005693409126251936},\n",
       "  {'label': 'admiration', 'score': 0.005559689365327358},\n",
       "  {'label': 'fear', 'score': 0.004826841875910759},\n",
       "  {'label': 'nervousness', 'score': 0.004653627518564463},\n",
       "  {'label': 'desire', 'score': 0.0039973268285393715},\n",
       "  {'label': 'disappointment', 'score': 0.0038509401492774487},\n",
       "  {'label': 'amusement', 'score': 0.0031087114475667477},\n",
       "  {'label': 'embarrassment', 'score': 0.002306994516402483},\n",
       "  {'label': 'surprise', 'score': 0.0021534764673560858},\n",
       "  {'label': 'gratitude', 'score': 0.001993270358070731},\n",
       "  {'label': 'disgust', 'score': 0.0019795733969658613},\n",
       "  {'label': 'grief', 'score': 0.0018784780986607075},\n",
       "  {'label': 'disapproval', 'score': 0.0016043229261413217},\n",
       "  {'label': 'remorse', 'score': 0.0015874926466494799},\n",
       "  {'label': 'anger', 'score': 0.0013102799421176314},\n",
       "  {'label': 'love', 'score': 0.001141196466051042},\n",
       "  {'label': 'confusion', 'score': 0.001093654427677393},\n",
       "  {'label': 'curiosity', 'score': 0.0004433416179381311}]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classifier(\"I am walking now\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes this text as input and predicts the sentiment associated with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline doc: https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "pipeline tasks: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More then one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.6145071983337402},\n",
       " {'label': 'POSITIVE', 'score': 0.999642014503479}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We give a list to the classifier now\n",
    "results = classifier([\"My hovercraft is full of eels and that's it\", \"My borther won\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Add different text inputs with varying sentiments, run it, check the model's sentiment predictions, and explore how it assigns labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now select a specific model into your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SamLowe/roberta-base-go_emotions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model_name variable holds the name of the pre-trained model. In this case, it's \"twitter-roberta-base-sentiment-latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the model card: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization is the process of breaking down text into smaller **units** called **tokens**.\n",
    "\n",
    "- Tokens are the basic building blocks used by Transformers models to understand and process text.\n",
    "\n",
    "- Tokens can represent **words, subwords, or even individual characters**, depending on the model's vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source image: https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"AutoModelForSequenceClassification\" adapts to various model architectures automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using from_pretrained, we are loading a pre-trained model and tokenizer specified by the model_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our sentiment analysis classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens to inputs IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"Another cool sentence to demonstrate something.\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokenizer(\"Another cool sentence to demonstrate something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens:['another', 'cool', 'sentence', 'to', 'demonstrate', 'something', '.']\n",
      " Token IDs: [2178, 4658, 6251, 2000, 10580, 2242, 1012]\n",
      " input_ids:{'input_ids': [101, 2178, 4658, 6251, 2000, 10580, 2242, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f' Tokens:{tokens}')\n",
    "print(f' Token IDs: {token_ids}')\n",
    "print(f' input_ids:{input_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "Test different tokenizers, select models from the hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some:\n",
    "\n",
    "https://huggingface.co/SamLowe/roberta-base-go_emotions\n",
    "\n",
    "https://huggingface.co/bert-base-uncased\n",
    "\n",
    "Some more... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this to answer the exercise\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "tokens = tokenizer.tokenize(\"Alex went to Gorge's house! And that is Cooool!\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokenizer(\"Another cool sentence to demonstrate something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens:['Alex', 'Ġwent', 'Ġto', 'ĠGorge', \"'s\", 'Ġhouse', '!', 'ĠAnd', 'Ġthat', 'Ġis', 'ĠCo', 'o', 'ool', '!']\n",
      " Token IDs: [16804, 439, 7, 31407, 18, 790, 328, 178, 14, 16, 944, 139, 8110, 328]\n",
      " input_ids:{'input_ids': [0, 21518, 3035, 3645, 7, 8085, 402, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f' Tokens:{tokens}')\n",
    "print(f' Token IDs: {token_ids}')\n",
    "print(f' input_ids:{input_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Another cool sentence to demonstrate something.\",\n",
    "           \"All I need is two sentences.\"]\n",
    "batch = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors=\"pt\") #pt for pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "All our sample will have the same length (necessity for the model) - tensors must have the same shape.\n",
    "```\n",
    "padding=True and truncation=True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[    0, 21518,  3035,  3645,     7,  8085,   402,     4,     2],\n",
      "        [    0,  3684,    38,   240,    16,    80, 11305,     4,     2]])}\n"
     ]
    }
   ],
   "source": [
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a dictionary with keys 'input_ids' and 'attention_mask', with two tensors the 'input ids' tensor and the 'attention_mask' tensor.\n",
    "input_ids are unique ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T01:28:56.175042Z",
     "iopub.status.busy": "2023-10-05T01:28:56.174286Z",
     "iopub.status.idle": "2023-10-05T01:28:56.235479Z",
     "shell.execute_reply": "2023-10-05T01:28:56.234562Z",
     "shell.execute_reply.started": "2023-10-05T01:28:56.175013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.9261,  4.2183],\n",
      "        [ 2.8756, -2.4102]]), hidden_states=None, attentions=None)\n",
      "\n",
      "tensor([[2.9026e-04, 9.9971e-01],\n",
      "        [9.9496e-01, 5.0377e-03]])\n",
      "\n",
      "tensor([1, 0])\n",
      "['POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "# Prevent gradient computation (no need to compute gradients during inference)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch) \n",
    "    print(outputs)\n",
    "    print('')\n",
    "    predictions = torch.softmax(outputs.logits, dim=1)      # Apply softmax to convert model logits to probabilities\n",
    "    pprint(predictions)\n",
    "    print('')\n",
    "    labels = torch.argmax(predictions, dim=1)              # Find the index of the class with the highest probability for each example\n",
    "    pprint(labels)\n",
    "    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
    "    pprint(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T01:28:56.237212Z",
     "iopub.status.busy": "2023-10-05T01:28:56.236846Z",
     "iopub.status.idle": "2023-10-05T01:28:56.244170Z",
     "shell.execute_reply": "2023-10-05T01:28:56.243187Z",
     "shell.execute_reply.started": "2023-10-05T01:28:56.237180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[0.0000, 1.0000],\n",
      "        [0.9900, 0.0100]])\n"
     ]
    }
   ],
   "source": [
    "# Define the number of decimal places to round to\n",
    "decimal_places = 2\n",
    "# Round the probabilities\n",
    "rounded_probabilities = torch.round(predictions * 10**decimal_places) / (10**decimal_places)\n",
    "# Print the rounded probabilities\n",
    "print('')\n",
    "pprint(rounded_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T01:28:56.246373Z",
     "iopub.status.busy": "2023-10-05T01:28:56.245722Z",
     "iopub.status.idle": "2023-10-05T01:28:57.751592Z",
     "shell.execute_reply": "2023-10-05T01:28:57.750613Z",
     "shell.execute_reply.started": "2023-10-05T01:28:56.246340Z"
    }
   },
   "outputs": [],
   "source": [
    "save_directory = \"your_dir\"\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model. save_pretrained(save_directory)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
