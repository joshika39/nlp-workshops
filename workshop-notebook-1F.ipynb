{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a sentiment analysis pipeline with HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.34.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joshh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#for colab\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "classifier = pipeline(\"text-classification\", model=model_name, top_k=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a \"Sentiment Analysis\" **classifier** using the pipeline function provided by the Hugging Face Transformers library. This function allows us to easily use pre-trained models for various natural language processing (NLP) tasks, like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'neutral', 'score': 0.7125529646873474},\n",
       "  {'label': 'approval', 'score': 0.13431048393249512},\n",
       "  {'label': 'realization', 'score': 0.07332759350538254},\n",
       "  {'label': 'joy', 'score': 0.05254011228680611},\n",
       "  {'label': 'relief', 'score': 0.026701610535383224},\n",
       "  {'label': 'optimism', 'score': 0.012795311398804188},\n",
       "  {'label': 'excitement', 'score': 0.011233342811465263},\n",
       "  {'label': 'pride', 'score': 0.009617138653993607},\n",
       "  {'label': 'caring', 'score': 0.009234932251274586},\n",
       "  {'label': 'sadness', 'score': 0.008024858310818672},\n",
       "  {'label': 'annoyance', 'score': 0.005693409126251936},\n",
       "  {'label': 'admiration', 'score': 0.005559689365327358},\n",
       "  {'label': 'fear', 'score': 0.004826841875910759},\n",
       "  {'label': 'nervousness', 'score': 0.004653627518564463},\n",
       "  {'label': 'desire', 'score': 0.0039973268285393715},\n",
       "  {'label': 'disappointment', 'score': 0.0038509401492774487},\n",
       "  {'label': 'amusement', 'score': 0.0031087114475667477},\n",
       "  {'label': 'embarrassment', 'score': 0.002306994516402483},\n",
       "  {'label': 'surprise', 'score': 0.0021534764673560858},\n",
       "  {'label': 'gratitude', 'score': 0.001993270358070731},\n",
       "  {'label': 'disgust', 'score': 0.0019795733969658613},\n",
       "  {'label': 'grief', 'score': 0.0018784780986607075},\n",
       "  {'label': 'disapproval', 'score': 0.0016043229261413217},\n",
       "  {'label': 'remorse', 'score': 0.0015874926466494799},\n",
       "  {'label': 'anger', 'score': 0.0013102799421176314},\n",
       "  {'label': 'love', 'score': 0.001141196466051042},\n",
       "  {'label': 'confusion', 'score': 0.001093654427677393},\n",
       "  {'label': 'curiosity', 'score': 0.0004433416179381311}]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classifier(\"I am walking now\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes this text as input and predicts the sentiment associated with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline doc: https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "pipeline tasks: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More then one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.6145071983337402},\n",
       " {'label': 'POSITIVE', 'score': 0.999642014503479}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We give a list to the classifier now\n",
    "results = classifier([\"My hovercraft is full of eels and that's it\", \"My borther won\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Add different text inputs with varying sentiments, run it, check the model's sentiment predictions, and explore how it assigns labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now select a specific model into your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SamLowe/roberta-base-go_emotions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model_name variable holds the name of the pre-trained model. In this case, it's \"twitter-roberta-base-sentiment-latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the model card: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization is the process of breaking down text into smaller **units** called **tokens**.\n",
    "\n",
    "- Tokens are the basic building blocks used by Transformers models to understand and process text.\n",
    "\n",
    "- Tokens can represent **words, subwords, or even individual characters**, depending on the model's vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source image: https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"AutoModelForSequenceClassification\" adapts to various model architectures automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using from_pretrained, we are loading a pre-trained model and tokenizer specified by the model_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our sentiment analysis classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens to inputs IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"Another cool sentence to demonstrate something.\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokenizer(\"Another cool sentence to demonstrate something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens:['another', 'cool', 'sentence', 'to', 'demonstrate', 'something', '.']\n",
      " Token IDs: [2178, 4658, 6251, 2000, 10580, 2242, 1012]\n",
      " input_ids:{'input_ids': [101, 2178, 4658, 6251, 2000, 10580, 2242, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f' Tokens:{tokens}')\n",
    "print(f' Token IDs: {token_ids}')\n",
    "print(f' input_ids:{input_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "Test different tokenizers, select models from the hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some:\n",
    "\n",
    "https://huggingface.co/SamLowe/roberta-base-go_emotions\n",
    "\n",
    "https://huggingface.co/bert-base-uncased\n",
    "\n",
    "Some more... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 27.6kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████| 570/570 [00:00<00:00, 578kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████████████████████████████████████| 232k/232k [00:00<00:00, 1.69MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████████████████████████████████████| 466k/466k [00:00<00:00, 2.59MB/s]\n"
     ]
    }
   ],
   "source": [
    "#uncomment this to answer the exercise\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Alex went to Gorge's house! And that is Cooool!\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokenizer(\"Another cool sentence to demonstrate something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens:['alex', 'went', 'to', 'gorge', \"'\", 's', 'house', '!', 'and', 'that', 'is', 'co', '##oo', '##ol', '!']\n",
      " Token IDs: [4074, 2253, 2000, 14980, 1005, 1055, 2160, 999, 1998, 2008, 2003, 2522, 9541, 4747, 999]\n",
      " input_ids:{'input_ids': [101, 2178, 4658, 6251, 2000, 10580, 2242, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f' Tokens:{tokens}')\n",
    "print(f' Token IDs: {token_ids}')\n",
    "print(f' input_ids:{input_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Another cool sentence to demonstrate something.\",\n",
    "           \"All I need is two sentences.\"]\n",
    "batch = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors=\"pt\") #pt for pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "All our sample will have the same length (necessity for the model) - tensors must have the same shape.\n",
    "```\n",
    "padding=True and truncation=True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[  101,  2178,  4658,  6251,  2000, 10580,  2242,  1012,   102],\n",
      "        [  101,  2035,  1045,  2342,  2003,  2048, 11746,  1012,   102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a dictionary with keys 'input_ids' and 'attention_mask', with two tensors the 'input ids' tensor and the 'attention_mask' tensor.\n",
    "input_ids are unique ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-5.7059, -6.0613, -5.9658, -5.0061, -4.3712, -6.9165, -5.9688, -6.5262,\n",
      "         -6.8372, -5.5885, -5.4265, -6.1034, -7.0587, -6.1179, -6.4271, -6.6725,\n",
      "         -7.5742, -6.2602, -6.7766, -7.8902, -6.2853, -7.8410, -5.0055, -7.8402,\n",
      "         -7.6885, -5.9437, -6.8205,  3.4048],\n",
      "        [-5.7238, -6.0326, -5.7438, -4.8020, -4.5821, -6.9556, -5.8810, -6.3498,\n",
      "         -6.7756, -5.5713, -5.4902, -5.9506, -6.9905, -5.9530, -6.3645, -6.7789,\n",
      "         -7.5721, -6.2770, -6.8283, -7.8356, -6.3383, -7.8621, -5.1717, -7.9430,\n",
      "         -7.7602, -5.9722, -6.5982,  3.4492]]), hidden_states=None, attentions=None)\n",
      "\n",
      "tensor([[1.1023e-04, 7.7266e-05, 8.5000e-05, 2.2194e-04, 4.1876e-04, 3.2850e-05,\n",
      "         8.4748e-05, 4.8536e-05, 3.5563e-05, 1.2397e-04, 1.4576e-04, 7.4074e-05,\n",
      "         2.8496e-05, 7.3010e-05, 5.3591e-05, 4.1929e-05, 1.7019e-05, 6.3329e-05,\n",
      "         3.7786e-05, 1.2407e-05, 6.1756e-05, 1.3033e-05, 2.2208e-04, 1.3044e-05,\n",
      "         1.5180e-05, 8.6907e-05, 3.6161e-05, 9.9777e-01],\n",
      "        [1.0359e-04, 7.6068e-05, 1.0153e-04, 2.6039e-04, 3.2446e-04, 3.0224e-05,\n",
      "         8.8517e-05, 5.5389e-05, 3.6186e-05, 1.2066e-04, 1.3085e-04, 8.2568e-05,\n",
      "         2.9187e-05, 8.2366e-05, 5.4582e-05, 3.6065e-05, 1.6316e-05, 5.9575e-05,\n",
      "         3.4328e-05, 1.2536e-05, 5.6034e-05, 1.2209e-05, 1.7992e-04, 1.1260e-05,\n",
      "         1.3518e-05, 8.0801e-05, 4.3206e-05, 9.9787e-01]])\n",
      "\n",
      "tensor([27, 27])\n",
      "['neutral', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "# Prevent gradient computation (no need to compute gradients during inference)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch) \n",
    "    print(outputs)\n",
    "    print('')\n",
    "    predictions = torch.softmax(outputs.logits, dim=1)      # Apply softmax to convert model logits to probabilities\n",
    "    pprint(predictions)\n",
    "    print('')\n",
    "    labels = torch.argmax(predictions, dim=1)              # Find the index of the class with the highest probability for each example\n",
    "    pprint(labels)\n",
    "    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
    "    pprint(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Define the number of decimal places to round to\n",
    "decimal_places = 2\n",
    "# Round the probabilities\n",
    "rounded_probabilities = torch.round(predictions * 10**decimal_places) / (10**decimal_places)\n",
    "# Print the rounded probabilities\n",
    "print('')\n",
    "pprint(rounded_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T01:28:56.246373Z",
     "iopub.status.busy": "2023-10-05T01:28:56.245722Z",
     "iopub.status.idle": "2023-10-05T01:28:57.751592Z",
     "shell.execute_reply": "2023-10-05T01:28:57.750613Z",
     "shell.execute_reply.started": "2023-10-05T01:28:56.246340Z"
    }
   },
   "outputs": [],
   "source": [
    "save_directory = \"your_dir\"\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model. save_pretrained(save_directory)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
